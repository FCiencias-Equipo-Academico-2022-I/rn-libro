\section{Compuertas lógicas con neuronas}
%operaciones lógicas

Aquí se muestra como se puede utilizar un perceptrón para simular compuertas lógicas tales como el or, not, and.

Para simular la compuerta \emph{not}, como está es una función boolean de \(B \rightarrow  B\), talque $not(x) = -x$ entonces en un plano de dos dimensiones, la podemos representar con dos puntos, el  $p_{1} = (0,0)$  y $p_{2} = (1,0)$donde $p_{1}$ representa cuando $not(0) = 1$, $p_{1}$ representa cuando $not(0) = 1$. Teniendo el espacio de la función definido lo que nos toca es, separar el plano para clasificar las entradas, este claramente se puede separar con un linea vertical, o con lineas con pendiente $1$ o $-1$. Para esta función solo necesitamos de una entrada y un sesgo (bias), donde la entrada la combinaremos con un peso, este peso lo asignaremos a tanteo (por la sencillez de la operación). Así el peso $w_{1} = -1$  y el peso asignado al bias sera $w_{0} = 0.5$, ahora con esto datos podemos:

\begin{itemize}
 \item Hacer la función de propagación donde $h(x) =  (x * -1) + (0.5) * 1 = 0.5 - x$
 \item Haccer la función de activación escalón $a(x) = sgn(h) = sgn(0.5-x)$ 
 \item Dar la salida donde $s(1) = sgn(0.5-1) = 0$ y $s(0) = sgn(0.5-0) =  1$, en este caso la salida es la identidad de la activación.
\end{itemize}

\begin{table}[H]
 \centering
\begin{tabular}{ | c | c |c | } 
 \hline
 $x$ &  $h$ & $s$ \\
 \hline
 $0$ &  $0.5$ & $1$ \\
 \hline
 $1$ &  $-1.5$ & $0$ \\
\hline
\end{tabular}
 
\end{table}


Algo similar va a pasar con la compuerta \emph{and} y \emph{or} donde al necesitar de dos entradas para la compuerta, asignaremos dos entradas para el perceptrón igualmente y las representaremos en el plano con cuatro puntos, donde cada punto representa una instancia y se le asigna valor positivo o negativo en el plano, así pues para el and tenemos los puntos $p_{1} = (0,0)$, $p_{2} = (0,1)$ , $p_{3} = (1,0)$, negativos y $p_{4} = (1,1)$ el único positivo. Así nos damos cuenta que necesitamos una recta con pendiente negativa y fuera del origen, que nos separe estás clases de puntos. Por tanto para el bias le asignamos un peso de $w_{0} = -1.5$, $w_{1} = 1$ y $w_{2} = 1$, con estos datos podemos: 


\begin{itemize}
 \item Hacer la función de propagación donde $h((x_{1},x_{2})) =  (x_{1} * 1) + (x_{2} *1) + (-1.5) * 1 = x_{1} + x_{2} - 1.5$
 \item Haccer la función de activación escalón $a((x_{1},x_{2})) = sgn(h) = sgn(x_{1} + x_{2} - 1.5)$ 
 \item Dar la salida donde $s = a(\vec{x})$, es la identidad de la activación.
\end{itemize}

\begin{table}[H]
 \centering
\begin{tabular}{ | c | c| c |c | } 
 \hline
 $x_{1}$ & $x_{2}$ & $h$ & $s$ \\
 \hline
 $0$ & $0$ & $-1.5$ & $0$ \\
 \hline
 $0$ & $1$ & $-0.5$ & $0$ \\
 \hline
 $1$ & $0$ & $-0.5$ & $0$ \\
 \hline
 $1$ & $1$ & $0.5$ & $1$ \\
\hline
\end{tabular}
 
\end{table}


Para la compuerta \emph{or} es algo muy similar pues podemos igualmente representar la función con cuatro puntos en el espacio cada uno representando una instancia, solo que ahora tres de estos puntos serán positivos y solo uno negativo, los puentos positivos serían $p_{2} = (0,1)$ , $p_{3} = (1,0)$ y $p_{4} = (1,1)$, mientras que $p_{1} = (0,0)$ negativo, el plano lo podemos dividir con una linea recta con pendiente negativa, así le asignamos $w_{0} = - 0.5$, $w_{1} = 1$ y $w_{2} = 1$, con esto hacemos los paso que ya sabemos:

\begin{itemize}
 \item Hacer la función de propagación donde $h((x_{1},x_{2})) = (x_{1} * 1) + (x_{2} *1) + (-0.5) * 1 = x_{1} + x_{2} + 0.5$
 \item Haccer la función de activación escalón $a((x_{1},x_{2})) = sgn(h) = sgn(x_{1} + x_{2} + -0.5)$ 
 \item Dar la salida donde $s = a(\vec{x})$, es la identidad de la activación.
\end{itemize}

\begin{table}[H]
 \centering
\begin{tabular}{ | c | c| c |c | } 
 \hline
 $x_{1}$ & $x_{2}$ & $h$ & $s$ \\
 \hline
 $0$ & $0$ & $-0.5$ & $0$ \\
 \hline
 $0$ & $1$ & $0.5$ & $1$ \\
 \hline
 $1$ & $0$ & $0.5$ & $1$ \\
 \hline
 $1$ & $1$ & $-1.5$ & $1$ \\
\hline
\end{tabular}
 
\end{table}


Tomando el hecho que en la naturaleza las neuronas van pasando información en una estructura que forma niveles de abstración, esto lo modelamos como capas de neuronas conectadas entre sí, cada capa haciendo su trabajo de abstración.
El perceptrón simple es un modelo neuronal unidireccional, una capa de entrada y otra de salida, que por si solo no puede separar todas la funciones lógicas pues tenermos el XOR, para resolver esto usaron perceptrones multicapa que se explicará más adelante en el curso.  
