\section{Propagación hacia adelante para el perceptrón multicapa}

Ahora para el modelo de una red en general tenemos nuestra arquitectura base que va a ser una red en capas también se le conoce como el perceptron multicapa (tipo feed-forward) en esta primera versión tenemos la capa de entrada que realmente solo recibe las entradas y el sesgo. Las salidas de cada capa sirven de entradas a la capa inmediatamente posterior en la red multicapa. Por lo general todas las salidas de una capa se distribuyen a todas las neuronas de la siguiente capa, formando capas completamente conectadas (fully-connected layers).
Cuando la red multicapa incluye más de una capa oculta, se dice que la red neuronal es profunda [deep neural network].
Los niveles de actividad de las neuronas de cada capa vienen dados por una función de activación no lineal de los niveles de actividad de las neuronas de la capa inferior.

(Insertar imagen de red).

Anteriormente habiamos estado asignando pesos a ojo/intuición, esto porque eran pocos los valores de entrada, esto normalmente no es así y vamos a desconocer en la totalidad los pesos necesarios que se aproximen a nuestra función objetivo.  

La red neuronal es en sí, es una función que nos va permitir pasar un vector de n dimesiones a uno de m dimensiones, con n la cantidad de características en nuestros datos y m la cantidad de características que necesitamos obtener.  

Entonces vamos a utilizar un tipo de codificación que se utiliza para la clasificación, llamado One-hot encoding, donde nos vamos a enforcar en el valor más elevado en nuestros resultados. Ahora nuestros problemas involuran más de dos clases, donde cada renglón de nuestra matriz de salida nos va indicar si pertence o no a una clase, algun ejemplar dado. Así tenemos cada dimensión en la matriz va a representar una clasificación, por ejemplo si queremos distinguier en una imagen entre un coche, casa, animal lo podríamos codificar de la siuiente forma:

$$
Salida 1 = coche =
\begin{bmatrix}
  1 & 0 & 0 \\
  \end{bmatrix}
\begin{bmatrix}
  1 \\ 
  0\\ 
  0 \\
  \end{bmatrix}
$$

$$
Salida 2 = casa =
\begin{bmatrix}
  0 & 1 & 0 \\
  \end{bmatrix}
\begin{bmatrix}
  0 \\
  1 \\
  0 \\
  \end{bmatrix}
$$
$$
Salida 3 = vaca =
\begin{bmatrix}
  0 & 0 & 1 \\
  \end{bmatrix}
\begin{bmatrix}
  0 \\ 
  0 \\ 
  1 \\
  \end{bmatrix}
$$

Toda la parte del aprendizaje, de reconocer distintos patrones, obtención de características varias, va a quedar entre \emph{la capa de entrada y las capas ocultas}, para que al final nos quedemos únicamente con el problema de separar las clases unas de otras tantas sean necesarias, \emph{en la capa de salida} y tengamos nuestra información clasificada. Así cada neurona de salida es una clase, especifica, entonces si se activo esa neurona nos indica que nuestro ejemplar pertence a dicha clase. Es importante no escatimar en el número de perceptrones necesarios para la clasificación a la salida, pues esto podría resultar en asignación conjunta de características, que se podrían interpretar como similitudes entre clases y en caso de no existir, dificultaarian enormemente la distinción entre características y generar fallos a la red. Siempre asignar tantas dimenciones de salida (perceptrones) como clases.

Los calculos para cada capa intermedia realmente son las mismas que hemos visto anteriormente representandose de la siguiente forma:
\begin{equation}
 a_{j}^{(l+1)} = g \left( \sum_{i} w_{ij}^{(l)}a_{i}^{(l)} \right)
\end{equation}

Con $l+1$ representando el número de capa a la que vamos, $g$ la función activación (normalmente podemos usar la función sigmoide) y $a_i$ los valores de las entradas. 


