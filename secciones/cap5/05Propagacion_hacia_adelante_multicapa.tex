\section{Propagación hacia adelante multicapa}


Ahora para el modelo de una red en general tenemos otra vez nuestra nuestra arquitectura base que va a ser una red en capas también se le conoce como el perceptor multicapa en esta primera versión tenemos la capa de entrada que decíamos es la capa de chocolate porque realmente no son percepciones solamente reciben sesgos si los vamos a ocupar y nuestros datos o características para nuestros ejemplares y aquí tenemos nuestros datos de entrada esto que estamos dibujando aquí de hecho es una función una función que viene de x n y evaluado hacia digamos xy son funciones sin moldes podríamos pensar en los reales donde tendremos que llamar aquí por ejemplo m y queremos referirnos al número de neuronas en la capa de salida. 
Entonces cómo se evalúa esta función pues ingresamos nuestros datos x a la izquierda vamos propagando hacia la derecha calculando valores intermedios hasta obtener nuestra salida en tantas dimensiones como queramos tener aquí en este ejemplo por ejemplo aquí en ese día 4 aquí el 3 podría ser ahora para ir contando capas vamos a pensar que en la de entrada para hacerlo ser de la capa 0 la capa oculta le podemos llamar a 1 la siguiente capa le podemos llamar a 2 observemos entonces que es posible continuar esta definición agregando más neuronas en la parte de adelante agregando sesgos si eso es lo que queremos y agregando las conexiones correspondientes bueno nadie mucho y entonces lo único que haríamos es repetir el patrón necesitaríamos un nuevo set de conexiones entre las neuronas de la capa anterior y la siguiente y un nuevo valor de activación tantas como nosotros queramos entonces las matemáticas que vamos a plantear aquí se van a generalizar rápidamente a la inclusión de mayores capas sí sí tiene su utilidad como lo iremos viendo más adelante sin embargo también podremos ver qué para poder aproximar absolutamente cualquier función con el mapeo de aquí para acá en realidad lo único que necesitaríamos teóricamente es agregar neuronas en la capa de enmedio en teoría no necesitamos más capas adelante un poco porque va a ser necesario más adelante otras arquitecturas bueno pues porque en toda esta parte lo único que hemos visto es cómo evaluar la función que define la red neuronal si ya conocemos los pesos pero qué sucede cuando no conocemos los pesos necesitamos encontrar los pesos correctos para que esta arquitectura aproxima en la función que nosotros queremos si los tenemos que buscar entonces ahí es donde puede ser más fácil o más difícil dependiendo de la inclusión de otras capas o de otras arquitecturas entonces bueno por el momento terminemos simplemente de plantear la parte de cómo se ve la función de la red neuronal entonces decíamos el formato es vamos a hablar de los valores de activación por eso tenemos estas mayúsculas y cada una de las capas las vamos a ir numerando de izquierda a derecha después a la derecha vamos a ponerle el índice a la matriz de pesos de las conexiones que hay entre la capa anterior hacia la que vamos entonces este cero porque toma como entradas a los valores de activación cero recordemos entonces que cada uno de estos valores van a estar hechos por matrices donde las hadas pueden tener su ejemplar sobre los renglones y podríamos evaluar de una vez para varios ejemplares si los agregamos como renglones en el caso de los pesos recordemos que estamos poniendo los pesos que se conectan con cada una de las neuronas en la forma de columnas y repetimos y significa eso de que yo puedo tener a cada uno rm aquí podría querer más de una neurona vamos a definir aquí entonces un tipo de codificación que se utiliza para los problemas de clasificación y se le conoce en inglés como one hot en coding.

One hote encoding.
Falta todo esto.
