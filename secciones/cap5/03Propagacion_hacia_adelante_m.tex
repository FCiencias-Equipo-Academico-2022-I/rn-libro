\section{Propagación hacia adelante vectorizada (con matrices)}

 La sección anterior si bien nos da la idea somera de como van a ser las operaciones para el aprendizaje ahora veamos lo de forma matricial. Esto nos ayudará a en un momento dado escrbirlo en el lenguaje de nuestra convencia.
 
 Tomemos en cuenta de nuevo a la red neuronal  de la sección anterior y recordamos que  tenemos nuestros valores de entrada 0 y hoy vamos a imaginarnos  exactamente como están escritos aquí y vamos a pensar que los metemos todos en un solo vector que va a entrar precisamente a la capa de entrada que en este momento la llamamos a y eso es exactamente lo que vamos a tener aquí pensemos en nuestras entradas como un vector ahora para poder hacer los siguientes cálculos vamos a acomodar ahora los pesos correspondientes a cada perceptor para el primer perceptor tomamos los pesos que están conectados desde la neurona de origen a 0 a 1 a 2 hacia la neurona oculta 1 por eso los índices 0 1 1 1 2 1 índice origen hacia índice destino y vamos a colocar todos esos en un renglón de la matriz de pesos aquí estaría la representación de nuestro primer perceptor para el segundo percepción vamos a tomar también su respectivo conjunto de pesos y le vamos a colocar en el segundo renglón de la matriz observen entonces que los valores que estoy tomando menos 0.5 1 y 1 que son éstos menos 0.5 uno y uno son términos que había utilizado anteriormente para hacer mi primer cálculo bueno 0.51 y ahora pasó el segundo perceptor 1.511 1.5 menos 11 son también los pesos de las conexiones con este otro persona 1.5 menos uno y menos uno entonces ahí está cada renglón es un perceptor qué pasa ahora si tomamos las dos matrices y colocamos los pesos acá en nuestro valor de activación del lado derecho observen que entonces la forma de resolver la operación de multiplicación para matrices me dice que voy a multiplicar menos 0.5 por 1 uno por cero y uno por uno muy bien regresemos acá y recordemos que este siempre vale uno éste valía cero y éste valía uno entonces lo que tengo es exactamente la misma operación que tenía que arriba y lo mismo me va a pasar con los otros dos valores 1.5 menos son en menos uno pues aquí es el menos el 1.5 y los demás parece igualmente y entonces tenemos una misma entrada operando con los dos diferentes percepciones el resultado de haber hecho esta primera multiplicación me va a quedar en la parte de arriba que correspondería al valor de activación de mi primer perceptor en la cama oculta este por este me va a dar ahora al segundo perceptor que serían este y este recordemos entonces que este de acá el sesgo es por definición entonces por eso no lo tuvimos que introducir en nuestros cálculos para poder pasar a la siguiente etapa entonces lo que hacemos ahora es convertir este resultado de acá que es el vector que habíamos obtenido con esta multiplicación pero ya con la función de activación y ahora éste se va a convertir en nuestro nuevo vector de entrada entonces para obtener agua lo que haríamos sería lo siguiente habría que especificar ahora que estamos agregando el vector de sesgo le ponemos un 1 más tendríamos que obtener también los pesos y afectan a nuestro percepción en este caso copiamos este acá y hacemos la multiplicación y aplicamos nuestra función de activación y con eso ya podemos resolver la red observen que este formato nos puede funcionar en general para cualquier red y este le vamos a llamar la convención 1 básicamente la seguí para que fuera fácil visualizar la relación entre la red que tenemos a ver y la forma en la que estoy escribiendo los vectores el 10 1 y el 1 lo dejamos acá y vendría la siguiente en verdad la forma en que lo escribí ahorita solamente me va a funcionar cuando tengo un ejemplar que estoy alimentando ahora necesito modificarla un poco para poder trabajar con varios valores de entrada digamos que ya no nada más quiero evaluaré 0,1 digamos que quiero evaluar 0 0 0 1 1 0 y 1 existen varias formas equivalentes para hacerlo vamos a utilizar ahorita una que sea fácil de utilizar con la forma en que nos entregan las tablas de datos.
 
 Lo que nos intriga ahora es saber que vamos a hacer en el caso que tengamos varias entras a trabajar al mismo tiempo.
 
 Existen varias opciones una opción sería tomar esta misma representación e inmediatamente comenzar a añadir cosas acá por ejemplo podríamos decir que tendríamos el 100 aquí el 110 acá el 111 desarrollar todas nuestras matemáticas ver cómo acomodar nuestras matrices y al final obtener un resultado que tenga todos los resultados sobre las columnas se puede hacer y seguramente encontrarán quien lo haga así por el momento vamos a seguir otra convención y este le voy a llamar la convención 2 lo que estamos haciendo en la convención 2 es más bien poner nuestro ejemplar de manera horizontal como renglón de ventaja va a tener esto ahora cuando yo quiera poner los demás ejemplares bueno como se va a ver recordemos que este que tenemos a la izquierda son nuestros sesgos entonces va a ser una matriz con los primeros unos y después de eso voy a poder escribir mi tabla como estamos acostumbrados a verlos en circuitos digitales por ejemplo más adelante cuando estemos trabajando con redes neuronales sobre diferentes tipos de características como edades número de hijos tamaño de la casa etcétera etcétera entonces podremos llegar a tener un conjunto de datos donde hacia acá estamos enumerando todas las características estamos evaluando y así acá tenemos los ejemplares con los que vamos a trabajar muy bien entonces de hecho si ustedes les dieron una pequeña base en excel es precisamente así como se los van a dar en la parte de arriba está bien en los títulos que es lo que se está poniendo y es ahí abajo vienen los valores y que de renglón representa hay un empleado alguna estudiante etcétera etcétera entonces esta anotación es mucho más directa para la forma en que vamos a recibir la información y tal vez esté más fácil de interpretar qué está pasando con los renglones por eso vamos a utilizar esta versión muy bien entonces si ahora en nuestro ejemplar de entrenamiento viene de forma horizontal vamos a tener que trabajar al revés con los pesos cada percepción va a ser ahora una columna entonces ahora vemos colocados aquí los mismos pesos de los percepciones del ejercicio anterior bueno un ejemplo entonces vemos como esta matriz es en realidad la matriz transpuesta de la versión anterior entonces si quieren pasar entre una versión y otra en darnos hay que tener cuidado con las trans puestas si coloquemos así ahora los pesos para poder hacer la multiplicación también vamos a necesitar poner al revés los valores de entrada y los pesos ahora la a nos está quedando del lado izquierdo y los pesos del lado derecho ojo esta doble voy a estar no son lo mismo que relaciona y entre ésta y ésta de tres tandas de dibujar esto ahora porque aquí tenemos doble uva y aquí tenemos a w algo que les va a servir mucho cuando encuentren diferentes textos con diferentes convenciones es recordar esta regla matemática a transpuesta de respuesta es igual transpuesta muy bien entonces vamos a seguirlo de esta manera vamos a ponerla a la izquierda los pesos a la derecha pesos sobre las columnas y podremos repetir otra vez la notación la diferencia es que en esta ocasión los resultados ahora nos van a quedar sobre los renglones prueben hacer esto con un pipe y resuelvan lo para todas las posibles entradas del short y verán que pueden obtener acá al final su tablita con los resultados correctos no olviden cada vez que cambien de capa que a la izquierda hay que agregar los sesgos aquí tenemos entonces ya cómo se vería el final de la operación es poner ahora los resultados de activación de h aquí ahorita nada más está un ejemplar si quieren poner los demás solamente se agregan como más renglones ahora aquí están los pesos para la conexión con el último perfecto y realizamos la misma operación y obtenemos aquí la salida para este valor y aquí tuviéramos otros renglones entonces nos saldrían aquí también los otros resultados resumiendo entonces esta es la forma en la que veríamos nuestros datos de entrada en las cuatro entradas para la compuerta digital al agregar los sesgos es equivalente a agregar aquí al inicio una columna de 12 entonces podemos sustituir pesos [Música] tenemos aquí entonces los valores de activación del lado izquierdo los pesos del lado derecho y al realizar las multiplicaciones correspondientes lo que estamos obteniendo aquí son los valores de activación para la capa oculta para calcular h es lo que tenemos aquí aquí todavía no tenemos los sesgos para dar el siguiente paso observen que es básicamente como repetir este ciclo comenzaríamos con las haches siendo estos valores que están acá y luego habría que agregar sesgos calculan los pesos para la siguiente capa hacer la multiplicación etc vamos a hacer un rápido análisis de qué fue lo que pasó con la función sort también en el mundo matemático que pasó entre capa y capa recuerden que mencionaba varias veces que ya no iban a ser las mismas entradas cuando se aplicará el último and que se hubiera estado en la primera capa.

