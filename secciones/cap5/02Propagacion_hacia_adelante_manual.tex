\section{Propagación hacia adelante manual}

En está parte vamos a ver como podemos evaluar la red para la solución que se dio en la sección anterior. (Recordemos que tambíen se pueden dar otras soluciones para el XOR).

Entonces entonces el procedimiento matemático general para poder evaluar una red cuando tenemos más de un perceptron. Veamos primero lo que está pasando con el \textbf{or}, y el \textbf{nand}, por el momento no tomaremos en cuenta las neuronas de entrada $a$ puesto que solo son usadas para almacenar las entradas. Ahora si
los perceptrones son los dos que conforman la capa oculta, pensemos en cada uno de estos como una unidad, cada perceptron está compuesto por los elementos de entrada y su sesgo  

tenemos una vez más la fórmula para un percepción cualquiera valores de activación x sus pesos calculamos la suma y aplicamos la función de activación ahorita en forma general esta señal la de aquí la función sigmoide pero recordemos que podríamos tener aquí una función escalón o cualquier otra función de activación entonces qué va a pasar con todos los perceptores de la capa oculta bueno nuestro primer percepción de ejemplo era este sustituimos a 0 a1 y a2 por ejemplo nuestro 0 y 1 a 0 que es el beat de sesgo que es idénticamente 1 porque recordemos que para un org pues necesitamos una recta que no necesariamente pasa por el origen entonces ya que tenemos estos elementos quedaría evaluado este valor que sucede con el segundo perceptor bueno tiene la misma forma serían estos elementos en este la zona porque existen los tres de entrada sus pesos correspondientes alimentando a h 2 ahora observen que lo que tienen en común h1 h2 es que realmente están alimentados por los mismos datos de entrada aunque cada uno de ellos tiene su propio conjunto de pesos y realmente son evaluados independientemente entonces hasta aquí ya nos quedaría los valores de alimentación para la capa oculta si nosotros completamos estas operaciones una vez que ya tenemos este conjunto de valores podemos empezar a trabajar con el tercer perceptor que sería este sus valores de entrada van a estar dados por los valores de activación de h1 de h2 y por nuestro nuevo sesgo este recordemos que en realidad es un land entonces también vamos a necesitar algo que esté saliendo no necesariamente pase por el cero entonces habiendo agregado estos elementos otra vez tenemos la forma típica de un percepción con sus entradas acompañadas de sus respectivos pesos y eso es lo que vamos a tener aquí la fórmula se ve igual lo único que está sucediendo es que ahora los valores de entrada fueron los valores que obtuvimos en el cálculo de la cab anterior como estamos trabajando precisamente capa por capa primero entran en los valores con los que van a trabajar todos los percepciones después calculamos todos los que están aquí son independientes entre sí aunque tengan en común las mismas entradas y una vez que terminemos este cálculo podemos proceder a la siguiente capa por eso estamos hablando de un algoritmo llamado de alimentación hacia adelante hay que recorrerlo de izquierda hacia derecha para ir obteniendo los diferentes valores de activación en cada una de las capas observen que una característica de este tipo de arquitectura es que siempre me estoy conectando de las neuronas en la capa anterior hacia las neuronas en la capa siguiente no puedo tener en este momento de dzitás de regreso entonces ya que tenemos estos elementos ya obtuvimos al final la evaluación de un short por medio de tres percepciones más los sesgos correspondientes bien podemos resolverlo así uno por uno pero también podemos simplificar un poco esta fase así es que lo que vamos a hacer en él en la siguiente etapa es tratar de hacer todo esto en paralelo cambiando anotación de matriz de matrices.
