\section{Función de error: Entropía cruzada}
Vamos a introducir entonces a nuestra función de error para problemas de clasificar y la entropía cruzada tiene su origen en teoría de la información mide la cantidad de bits necesarios para identificar una clase dada la hipótesis de la red siendo que éstas provienen de la función de x este gtx vendría a ser precisamente los valores reales a nuestras etiquetas lo que ocurre en el mundo experimental esta sería la red neuronal que está tratando de codificar con bits la información de a qué clase pertenecen estos datos entonces lo que estamos tratando de predecir es si esta función se parece a esta entonces va a ser muy fácil codificar en cualquier este clase de estos datos utilizando esta función si no se parecen entonces necesitaríamos un montón de bits y va a ser muy eficiente básicamente no nos va a funcionar si lo queremos visualizar más gráficamente me gusta utilizar la siguiente explicación estos cheques que tenemos aquí son nuestras etiquetas es decir los valores correctos que nosotros creíamos vemos que en esta operación aparecen dos términos aparece y aparece uno menos y bueno si estamos tratando de un problema de clasificación recordemos que entonces nuestras neuronas de salida van a tener dos posibles valores 0 y 1 la respuesta correcta es 0 o es un como tenemos una función de activación logística en este caso nunca vamos a tener exactamente 0 o exactamente 1 pero sí nos vamos a acercar lo más que podamos dependiendo de los pesos que petaca recordemos que las magnitudes afectaban s entonces estos días van a representar precisamente esos valores 0 y 1 y lo que tenemos acá es el logaritmo de lo que obtuvimos vamos a considerar entonces los dos casos supongamos que teníamos un cero pero lo que predijo nuestra red fue uno lo que iba a suceder entonces bueno de estos dos términos observemos que si lleva el es cero este término simple y sencillamente no va a contar nos vamos a quedar únicamente con este en este caso pasa lo siguiente si lleva el es cero aquí es un 1 observen que realmente el error va a estar dado por esta función de acá si lo que obtuvimos aquí otra vez insistes cercano a uno porque tenemos una función logística entonces qué va a suceder con este logaritmo vamos a tener un número cercano a cero que ocurre con el logaritmo cuando nos vamos a cero bueno este le funciona carisma lo que está pasando es que nos estamos disparando hacia menos infinito observemos este signo menos que se coloca acá en la definición de entropía cruzada el efecto que va a ser entonces es que si queríamos un valor cero y obtuvimos algo cercano a uno bueno entre más nos acerquemos a uno esto más se nos va a disparar hacia infinito como quería uno una razón y nos estamos yendo para acá muy bien y pasa entonces si si queríamos un 0 y si obtuvimos un 0 bueno lo que vamos a tener aquí entonces es el logaritmo de 1 es cero entonces si la respuesta era correcta nuestra función de error va a valer cero perfecto otra ves como es una función sigmoide bueno nunca vamos a tener exactamente cero es aquí el error nunca va a ser exactamente cero pero va a estar cerca vamos a ver qué ocurre ahora en este otro caso bueno supongamos ahora que queríamos que valiera pero lo que obtuvimos fue algo cercano a cero en ese caso una vez más esto se convierte en uno entonces realmente la función de error está dada por el término de este lado tenemos entonces el logaritmo de algo cercano a 0 que se nos va a disparar hacia menos infinito multiplicado por el menos que tenemos acá afuera bueno pues vamos a completar de hecho el otro lado de esta gráfica lo que tenemos es que estamos hacia acá arriba y queríamos un valor 1 en el aire y obtuvimos un 1 bueno aquí se queda el 1 aquí nos queda el logaritmo de 1 que es 0 entonces vamos a estar hacia este lado de esta manera lo que podemos pensar entonces es que esta función de error tiende a ser lo siguiente si nos estamos equivocando se dispara hacia infinito y estamos en lo correcto vamos a estar muy cerca de cero cuando estamos hablando ya de un sistema complejo como una red neuronal con muchísimas neuronas varias capas varias neuronas en la capa de salida pues la gráfica en realidad no va a ser este paraboloide bonito va a ser una gráfica con bastantes rugosidades mínimos locales etcétera pero digámoslo así es menos rugosa que si hubiéramos utilizado diferencias al cuadrado entonces por eso vamos a utilizar esta función bueno ya leyendo los detalles de la anotación vamos a fijarnos entonces que estamos obteniendo un promedio sobre el número de ejemplares de entrenamiento o el número de ejemplares en general sobre los cuales estemos evaluando en este momento el error que comete nuestra red y eso va a ser m es por eso tenemos la suma desde uno hasta m de todas las contribuciones de error y 1 / m para que estemos hablando de un promedio s l es el número de neuronas y capa de salida recordemos que en general nuestro perceptor multicapa va a tener una neurona por cada clase que nosotros estemos tratando de evaluar entonces tiene sentido que sumemos el error para cada una de las clases con las cuales estamos trabajando y observemos que cada una de esas clases efectivamente está representada por algún valor 0.1 en su neurona correspondiente entonces aquí estamos sumando sobre todas las neuronas de salida que corresponde a todas las clases que estamos tratando de identificar con esta red y que es el valor deseado para la carísima neurona de salida y toma valores en 0.1 bueno entonces cada una de estas neuronas tiene que tener etiqueta corresponde y finalmente htc está es la red neuronal evaluada en el ejemplar y estima.
