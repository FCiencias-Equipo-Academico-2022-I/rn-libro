\section{Vectorización}
Vamos a poner la red mediante el uso de notación, con matrices y vectores y vamos a aprovechar, para generalizar nuestras fórmulas en la derivación matemática del grediente se hizo considerando que tuviéramos un solo ejemplar de entrenamiento pero realmente vamos a entrenar a nuestra red con varios ejemplares. 

Consideremos todos los datos involucrados durante el entrenamiento:
\begin{enumerate}
 \item Hay $S_{L}$ neuronas de salida, para las cuales se calculará el error.
 \item Se puede calcular el promedio del gradiente para m ejemplares simultáneamente.
 \item Si escribimos los datos en forma matricial, es posible paralelizar estos cálculos utilizando operaciones de matrices.
\end{enumerate}
Denotemos nuevamente nuestra función de error:
 \begin{equation}
  J (\Theta) = -\dfrac{1}{m}\left[\sum_{i=1}^{m}\sum_{k=1}^{s_{L}}y_{k}^{i} log( h_{\Theta}(x^i))_{k}+(1-y_{k}^{i})log(1- h_{\Theta}(x^i))_{k}  \right]  
 \end{equation}

Ahora con nuestros datos en forma matricial:
\begin{equation}
X = 
\begin{pmatrix}
x_{0}^{(1)} & \cdots & x_{n}^{(1)}\\
\vdots & \ddots & \vdots\\
x_{0}^{(m)} & \cdots & x_{n}^{(m)}\\
\end{pmatrix}
\end{equation}


\begin{equation}
Y = 
\begin{pmatrix}
y_{0}^{(1)} & \cdots & y_{S_{L}}^{(1)}\\
\vdots & \ddots & \vdots\\
y_{0}^{(m)} & \cdots & y_{S_{L}}^{(m)}\\
\end{pmatrix}
\end{equation}


\begin{equation}
\Theta^{(l)} = 
\begin{pmatrix}
\Theta_{01} & \cdots & \Theta_{0s_{l}}\\
\vdots & \ddots & \vdots\\
\Theta_{(s_{(l-1)}1)} & \cdots & \Theta_{(s_{(l-1)}Sl)}\\
\end{pmatrix}
\end{equation}

Entonces vamos a agregar eso también tenemos otra vez escrita nuestra fórmula para el error de entropía cruzada pero ahora sí estamos agregando este término donde estamos calculando un promedio sumando los errores sobre todos los ejemplares de entrenamiento aparecen entonces estos índices extra y en la parte de arriba para indicar en qué ejemplar de entrenamiento nos encontramos y para poder empezar a trabajar con matrices vamos a empezar a ver cómo están escritos nuestros datos todo recordemos como habíamos dicho que íbamos a escribir nuestro nuestras entradas de entrenamiento la idea es que cada ejemplar es un renglón tenemos n características este x0 está pensando en que estamos agregando aquí los sesgos y después observemos lo que ocurre con nuestras etiquetas de clasificación recordemos que en redes neuronales utilizamos el one hot en coding entonces aunque si tenemos cinco clases tenemos cinco neuronas de salida y eso quiere decir que nuestra etiqueta tiene algo así este sería una etiqueta en la que la clase correcta es la que está en la tercera neurona otro ejemplar podría ser de esta manera y pues aunque solamente haya uno que sea distinto de cero la forma en la que van a venir empaquetadas nuestras respuestas correctas va a ser precisamente en forma de matriz donde tenemos sobre los renglones tantos bits como neuronas haya en la última capa y tenemos hacia abajo los m ejemplares de entrenamiento después recordemos cómo teníamos escritas las matrices de pesos teníamos por cada columna los pesos que contribuyen a la a una neurona en la siguiente capa entonces sobre cada renglón tendríamos a tener tantos pesos como neuronas haya en la capa cl y hacia abajo vamos a tener tantos pesos como neuronas había en la capa sl -1 y que contribuyeron al siguiente elemento en ese l así que tendríamos a las diferentes neuronas que van a estar en nuestra nueva capa eso es lo que va a hacer es que cuando multipliquemos x y z nos queden otra vez los valores de cada en la siguiente capa con los renglones correspondientes a los ejemplares de entrenamiento y horizontalmente los valores de activación de cada neurona en la siguiente bien entonces partes importantes ejemplares de entrenamiento hacia abajo para estos dos aquí el décima capa hacia acá está para l menos uno ahora están copiadas acá del lado izquierdo las fórmulas tal y como las obtuvimos ejemplar por ejemplar y lo que vamos a hacer ahora es escribirlas en forma matricial bueno que ya me adelanté ya se las escribí entonces va a ser más fácil si ahora explicó exactamente qué es lo que estamos viendo aquí para eso voy a utilizar ahora otro programa y aquí está vamos a utilizar aquí para que pueda dibujar qué es lo que está primero que teníamos en delta está en la capa no en la capa l bueno voy a dibujar aquí ahorita una pequeña red neuronal en este conector y entonces nos estamos preguntando por qué pasa con él aquí está él bueno lo que decíamos era que vamos a tener una de estas del estás por cada uno de nuestras neuronas de salida y lo que vamos a ver es qué tenemos los ejemplares hacia abajo y las neuronas horizontales los valores que tengo aquí verticalmente los acosté y es lo que tengo acá ya vimos también entonces que la llega realmente tiene varios bits horizontalmente y la sas también entonces si yo restó enrique menos acá observen que esto en realidad es un vector tengo una componente por cada neurona de salida entonces aunque aquí les saquemos componente por componente pues tengo uno de cada uno entonces de todo esto me va a salir todo un vector y si además considero que hay ejemplares de entrenamiento pues voy a tener m de esos entonces la forma más cómoda para acomodar eso va a ser la matriz delta l donde voy a restar dos matrices voy a restar la matriz que tiene esta forma menos la matriz l que de hecho tiene exactamente la misma forma la diferencia es que como ésta está evaluada con una logística pues los valores no son ceros y unos en realidad son valores entre 0 y 1 entonces cuando haga la resta pues voy a obtener otra vez una matriz con la misma forma pero pues con números entre 0 y 1 entonces delta l va a tener sl componentes hacia la derecha y m ejemplares dentro y entonces ya podemos ver porque simplemente basta con que yo reste menos en forma matricial y ya automáticamente tengo acomodados todas las del test que aquí tendría que haber calculado una por una y como estamos repitiendo eso para todos los ejemplares de entrenamiento pues ya tengo de una buena vez todos acomodados en la misma matriz y vamos a hacer ahora para el gradiente este es un efecto mucho mucho mucho muy interesante porque en primer lugar aquí este nos conviene el hecho de que tenemos varios ejemplares de entrenamiento y nosotros lo que queremos hacer es sacar un promedio sobre todos ellos entonces aparte de que afectan los signos menos entonces el signo menos se conserva el 1 entre m se conserva porque es para poder sacar promedio pero ahora vamos a tratar de matar varios pájaros de una pedrada observemos lo siguiente tenemos que multiplicar y sumar a todas las valores de activación por el delta observen que este es en la capa anterior este es el error que cometió en la capacidad ente el hacer las combinaciones de todos contra todos es cuando obtengo el efecto que ocurrió con cada peso pero quiero sumar estos productos para todos los ejemplares de entrenamiento y sumarlos bueno eso es prácticamente lo que hace una multiplicación de matrices ahora para poder hacer todo esto en un solo paso lo que tenemos que hacer es acomodar las acorde mente nos vamos a hacer lo siguiente ya habíamos dicho que esta delta que realmente tiene forma de matriz como ésta de tal manera que mira aquí hacia acá tenemos todos los es el es y así aquí abajo tenemos todos nuestros ejemplares de entrenamiento bueno y que sabemos de la aj y también son los valores de activación y queremos combinar cada j de cada ejemplar de entrenamiento con su respectiva que del mismo ejemplar de entrenamiento y después multiplicarlos y sumarlos bien entonces vamos a acomodar a la sas de la siguiente manera y vamos a poner hacia acá a los ejemplares de entrenamiento y así acá a los elementos son ordenadas iba a suceder si yo hago esto observen que se multiplican ejemplares de entrenamiento contra ejemplares de entrenamiento misma aj contra mismos del tak as pero de diferentes ejemplares de entrenamiento cada uno con su respectivo y además se suman para definir la coordenada que va a quedar aquí cuando yo tomé este contra la siguiente columna entonces otra vez vamos a estar multiplicando en ejemplares contra mi ejemplares los vamos a sumar y nos va a dar el dato que tengamos aquí los conservamos las columnas que teníamos acá s l cuando empiece a hacerlo con los renglones voy a tener entonces lo que ocurre con las s menos el c lm no son tan clones que teníamos acá y esto es sumamente interesante porque esto que está aquí debería de resultarnos un poco conocido sl sl - solo vamos a ver que teníamos acá sl sl - 1 tiene exactamente la misma forma en la matriz de pesos y no es coincidencia recordemos que es lo que estamos tratando de calcular estamos tratando de calcular el gradiente el ingrediente es la parcial con respecto a cada uno de los pesos entonces lo que acabamos de obtener es ese gradiente que tiene acomodadas cada una de las parciales en la posición que corresponde al mismo peso pero en la matriz de pesos bastante bonito muy bien entonces por eso cuando ponemos la matriz a él pero transpuesta para que tengamos ahora si los ejemplares de entrenamiento de forma horizontal entonces podemos obtener en una multiplicación de matriz todos los componentes del gradiente para los pesos en esa capa lo que ocurre con las siguientes bueno de hecho con los siguientes gradientes es que se van a hacer exactamente igual lo único que nos queda ligeramente entretenido es cómo vamos a calcular los errores de las capas intermedias hay entonces que observar lo siguiente en primer lugar las ventas van a tener la misma forma que éste es que primas entonces simple y sencillamente van a ser dos matrices donde se tienen que multiplicar componentes a componentes las vamos a poner con este símbolo que significa circo digo se llama cirque y significa multiplicación componente a componente entre estas dos matrices ahora como vamos a escribir ésta se parece a lo anterior pero ahora sobre lo que queremos es sumar es sobre las neuronas de salida recordemos que la contribución al error de una neurona de la capa intermedia pues es la suma sobre los errores a los que contribuyó en todas las neuronas en la capa siguiente entonces del ejercicio anterior ya debemos de ver como pista en que si queremos hacer una suma sobre estos elementos pues como que esto es lo que vamos a tener que tener en la parte horizontal de la primera matriz y vertical de la segunda entonces de aquí empiezan a salir precisamente los tips de cómo escribir estas 2 estás deltas pues ya tienen automáticamente de manera horizontal lo que está ocurriendo con cada una de las neuronas entonces se quedan exactamente iguales y lo que estaba ocurriendo con los pesos es que entonces vamos a crear los componentes de ese l sobre los renglones los necesitamos tomar la transpuesta para que cumpla con esa condición y en principio y básicamente ya quedaría que ese entonces está en notación extraña que tenemos en la parte de abajo recordemos que si estamos utilizando sesgos si estamos utilizando sesgos tenemos una neurona en esta capa que no está conectada en nadie con nadie en la parte de atrás entonces no existen pesos que hayan hecho que las neuronas de acá contribuyan al error de esta ésta no tenía error es un sesgo entonces aquí no hay nada que calcular por eso lo que tenemos que hacer en la parte de aquí es quitar a los elementos que corresponderían a las así pues el hecho de que los riesgos no están conectados con la capa anterior es básicamente lo que nos está diciendo es que quitemos de la matriz de pesos a todas las conexiones que parten de esta neurona porque estas conexiones pues no bueno ésta no va a estar contribuyendo cada día más bien las que estén en esta capa no están contribuyendo al error de esta neurona entonces lo que ocurra con esta neurona en este momento a estas no les importa realmente porque no están pasando para acá entonces por eso se quita estos elementos que estén aquí nos dice entonces que nos brincamos algo que está en el índice cero y nos vayamos nada más a los que están en el índice 1 entonces teniendo este cuidado de ir completando los datos sin los elementos de sesgo cuando no hay esos elementos presentes pues ya podemos implementar todo esto con operaciones ahora para poder ponerle un poco en limpio observamos que simplemente el que es diferente es el error cometido en la última capa nada más es yemen osa l después las fórmulas no sólo para la capa l menos uno sino que si tuviéramos más capas hacia atrás se verían idénticas entonces les podemos escribir así tenemos que el error va a ser esta delta l multiplicada por la matriz de pesos quitando las los que afectan en los sesgos x eje prima pero recordemos que ese prima pues tenía truco porque son los valores de activación que ya tenemos multiplicación componente a componente por 1 - estos valores de activación y finalmente para los componentes del gradiente ahora sí lo único que vamos a hacer era multiplicar los valores de activación de la capa anterior por los errores de la capa siguiente y eso nos iba a dar ahora sí la participación del gradiente para cada uno de los pesos y esto lo podemos repetir obteniendo uno de una matriz de estos por cada matriz de pesos que hayamos utilizado los con esta técnica básicamente obtenemos el gradiente de pequeños bloques hitos de material de matrices una matriz por cada matriz de pesos si quieren escribir el gradiente entonces como se escribe usualmente en cálculo es como un solo vector pues lo que hay que hacer es aplanar todas esas pequeñas matrices y poner cada una de las componentes en una componente del vector y nos va a quedar un vector gigantesco que tiene tantas tantas componentes como la suma de componentes en todas estas matrices bien y con eso quedaría todo el último detalle que les quiero mencionar es bueno este signo siempre se nos hace travesuras en esta versión recuerden que lo que acabo de calcular aquí es el gradiente y cuando nosotros hallamos descenso por el gradiente lo que necesitamos entonces es la dirección inversa entonces nada más finalmente entonces podemos decir en qué consiste el algoritmo de propagación hacia atrás lo único que estamos haciendo es obtener los ejemplares de entrenamiento las respuestas correctas necesitamos saber cuántas que pasan vamos a inicializar nuestras matrices de errores con ceros bueno podremos hacer lo mismo también con las del gradiente empezamos en la haciendo feedforward que es lo que nos indican estas primeras líneas asignando nuestros datos de entrenamiento a la primera capa la capa de chocolate a partir de ahí utilizamos feedforward para ir calculando todos los valores de activación de las siguientes capas hasta llegar a la última y a partir de ese momento podemos empezar a trabajar ahora sí con el entrenamiento de la red tomar en cuenta que era lo que queríamos para sustituir en los cálculos de los errores y los gradientes con las fórmulas que tenemos en la parte de atrás y nos importó precisamente las etiquetas y ya que tenemos ese entonces podemos actualizar ahora si nuestros pesos en paralelo tienen que actualizarse todos al mismo tiempo no para actualizar primero unos y después otros porque eso ya no es el gradiente en paralelo tenemos que actualizar tomando los pesos que ya teníamos menos alfa por el gradiente d función de error repitiendo esto el suficiente número de veces la idea es que eventualmente estos pesos nos permitan encontrar un mínimo de la función de error y eso sería todo en esta fórmula de hecho fue obtenida de andrew james en la fórmula para el error roselyn orvin presentan la derivación con diferencias del cuadrado en lo que se hizo aquí fue combinar ambas para obtener entonces la derivación pero ya con entropía cruzada y si quieren ver un poco más acerca de esta técnica pues pueden checar y sobre todo la parte de la interpreta.
