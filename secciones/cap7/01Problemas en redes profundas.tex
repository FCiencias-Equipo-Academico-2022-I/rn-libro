\section{Problemas en redes profundas}

Hasta este momento ya se ha visto como modelar desde una neurona hasta una red neuronal propiamente, se ha visto como hacer la configuración de tal manera que nos clasifique ejemplares, así como detectar el error en los pesos asignados y ajustarlo para mejores resultados. Ahora al momento de modelar redes neuronales profundas (en inglés deep neuronal networks, \emph{DNN}) tenemos que aceptar que el cálculo de estás asignaciones y ajustes requieren tiempo de cálculo, así llegando al \emph{primer problema, el tiempo de computación}. Y ademas nos podemos encontrar con \emph{el problema del sobreaprendizaje} este aparece cuando tenemos demasiadas hipótesis válidas pero no de suficientes datos para poder descartar todas menos la correcta. 

Cuando ajustamos los parámetros de una red neuronal a los datos del conjunto de entrenamiento, no podemos diferenciar las características realmente útiles de las irrelevantes o de las debidas al muestreo del conjunto de entrenamiento, por lo que siempre estamos
expuestos al riesgo de sobreajuste (overfitting en inglés).


 Métodos de regularización o la disminución de pesos  o la dispersión, se puede aplicar durante el entrenamiento para combatir el sobreajuste. Alternativamente, la regularización de dropout, omite aleatoriamente neuronas de las capas ocultas durante el entrenamiento. . Finalmente, los datos se pueden aumentar a través de métodos como el recorte y la rotación, de modo que los conjuntos de entrenamiento más pequeños se pueden aumentar de tamaño para reducir las posibilidades de sobreajuste.

Las DNN deben considerar muchos parámetros de entrenamiento, como el tamaño (número de capas y número de unidades por capa), la tasa de aprendizaje y los pesos iniciales. El barrido a través del espacio de parámetros para obtener parámetros óptimos puede no ser factible debido al costo en tiempo y recursos computacionales. Varios trucos, como el procesamiento por lotes (calcular el gradiente en varios ejemplos de entrenamiento a la vez en lugar de ejemplos individuales)  aceleran el cálculo, lo veremos más adelante. Las grandes capacidades de procesamiento de las arquitecturas de muchos núcleos (como GPU) han producido aceleraciones significativas en el entrenamiento.

Para mayor información del tema se sugiere leer el siguiente enlace:

CHAPTER 5 Why are deep neural networks hard to train? \url{http://neuralnetworksanddeeplearning.com/chap5.html}

El siguiente enlace les puede ayudar a ver como cada hiperparemetro puede afectar a los resultados de la red neuronal.
\url{https://quetzalcoatl.fciencias.unam.mx/moodle/mod/url/view.php?id=634&redirect=1}

Para ayudarnos a los calculos con el gradiente vea los siguientes enlaces:
\begin{itemize}
 \item \url{https://youtu.be/nUUqwaxLnWs}
 \item \url{https://youtu.be/FDCfw-YqWTE}
 \end{itemize}

