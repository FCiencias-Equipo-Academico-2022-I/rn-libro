\chapter[Independencia]{Dependencia e independencia probabilística}

\section{Independencia}

Dos eventos $\alpha$ y $\beta$ son independientes $P \models \alpha \perp \beta$\footnote{Se lee: $P$ satisface que $\alpha$ es independiente de $\beta$.} si:

\begin{align}
 P(\alpha \cap \beta) =& P(\alpha)P(\beta) \\
 P(\alpha|\beta) =& P(\alpha) \\
 P(\beta|\alpha) =& P(\beta)
\end{align}

Se dice que dos variables aleatorias $A$ y $B$ son \emph{independientes} $P \models A \perp B$ si se cumple que:

\begin{align}
 P(A,B) =& P(A)P(B) \\
 P(A|B) =& P(A) \\
 P(B|A) =& P(B)
\end{align}

No hay forma de representar el concepto de independencia con diagramas de Venn.  Obsérvese que independencia y exclusión ($A \cap B = \emptyset$) son dos conceptos distintos.

\section{Independencia condicional}

Se dice que dos eventos/variables aleatorias $E_1$ y $E_2$ son \emph{condicionalmente independientes} dado $F$, $P \models (E_1 \perp E_2 | F)$ si, dado que $F$ ocurre, la probabilidad condicional de que $E_1$ ocurra no cambia al obtenerse información sobre si $E_2$ ocurre o no.  Esto se escribe:

\begin{align}
 P(E_1|E_2F) =& P(E_1|F) \\
 P(E_2|E_1F) =& P(E_2|F) \\
\end{align}
o equivalentemente:
\begin{align}
 P(E_1E_2|F) = P(E_1|F)P(E_2|F)
\end{align}

Ejemplo: Se tienen dos monedas, una cargada de modo que 90\% del tiempo el resultado de un volado con ella será sol y otra moneda normal $Moneda = <Cargada, Normal>$.  Se elige una moneda al azar y se lanzan dos volados $Volado_i=<Sol, \acute{A}guila>$ [\fref{fig:condicionalMoneda}].  Inicialmente conocer el resultado del primer volado brinda información sobre la posibilidad de haber elegido la moneda cargada y por ende acerda del posible resultado del siguiente volado.  Sin embargo, si se sabe qué moneda se eligió, esa información es suficiente para conocer la probabilidad de cada resultado en el segundo volado, la información del primer volado ya no es relevante.  En este caso se dice que el resultado del segundo volado es independiente del resultado del primero, dado que se sabe qué moneda fue elegida.

\begin{figure}
 \centering
 \begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1, level distance = 1.5cm}]
  \node [ellipse,violetNode]{$Moneda$} [level distance=10mm,sibling distance=25mm]
    child { node [ellipse,cobaltNode] {$Volado_1$} }
    child { node [ellipse,cobaltNode] {$Volado_2$} };
 \end{tikzpicture}
 \caption{Un ejemplo de independencia condicional.  El resultado de cada volado con una moneda depende de si se eligió una moneda normal o una moneda cargada.  Si se sabe qué moneda se eligió, conocer el resultado de un volado ya no dice nada nuevo sobre el resultado de otro volado con la misma moneda.}\label{fig:condicionalMoneda}
\end{figure}


\section{Redes Bayesianas}

\begin{definition}[Red Bayesiana]
 Una \emph{Red Bayesiana} es:
 \begin{itemize}
  \item Una gráfica acíclica dirigida (GDA) $G$ cuyos nodos representan a las variables aleatorias $X_1,...,X_n$.
  \item Para cada nodo $X_i$ define una distribución de probabilidad condicional
  \begin{align}
   P(X_i|Padres_G(X_i))
  \end{align}
 \end{itemize}
\end{definition}

La red de Bayes representa un distribución de probabilidad conjunta para la cual se cumple, debido a la relación de independencia condicional entre las variables, que:
\begin{align}
 P(X_1,...,X_n) = \prod_i P(X_i|Padres_G(X_i))
\end{align}
A esta fórmula se le conoce como regla de la cadena para Redes Bayesianas.  Cuando un distribución de probabilidad conjunta $P$ satisface esta relación, se dice que $P$ \emph{se factoriza sobre} $G$.

\subsection{Independencia en redes bayesianas}


\begin{definition}[Ruta]
 Una \emph{ruta} $X_1-...-X_k$ es una secuencia de nodos que se encuentran conectados entre sí mediante una sola arista (no dirigida) en la gráfica.
\end{definition}

\begin{figure}
\centering
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1, level distance = 1cm}]
 \node [ellipse, cobaltNode] (D) {$X_1$};
 \node [ellipse, cobaltNode, right= of D] (I) {$X_3$};
 \node [ellipse, cobaltNode, below right= 1cm and 0.3cm of D] (G) {$X_2$};
 \node [ellipse, cobaltNode, below right= 1cm and 0.3cm of I] (S) {$X_4$};
 \node [ellipse, cobaltNode, below= of G] (L) {$X_5$};
 \draw [arrow,draw=red] (D) -- (G);
 \draw [arrow,draw=black] (I) -- (G);
 \draw [arrow,draw=black] (I) -- (S);
 \draw [arrow,draw=red] (G) -- (L);
\end{tikzpicture}\hspace*{2cm}
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1, level distance = 1cm}]
 \node [ellipse, cobaltNode] (D) {$X_1$};
 \node [ellipse, cobaltNode, right= of D] (I) {$X_3$};
 \node [ellipse, violetNode, below right= 1cm and 0.3cm of D] (G) {$X_2$};
 \node [ellipse, cobaltNode, below right= 1cm and 0.3cm of I] (S) {$X_4$};
 \node [ellipse, cobaltNode, below= of G] (L) {$X_5$};
 \draw [arrow,draw=red] (D) -- (G);
 \draw [arrow,draw=red] (I) -- (G);
 \draw [arrow,draw=red] (I) -- (S);
 \draw [arrow,draw=black] (G) -- (L);
\end{tikzpicture}
\caption{Izquierda: Ruta activa $X_1-X_2-X_5$.  Derecha: Ruta $X_1-X_2-X_3-X_4$ con estructura-v entre $X_1-X_2-X_3$, activada porque $X_2$ fue observada.}\label{fig:rutas}
\end{figure}



\subsubsection{Ruta activa}

Sea $\mathbb{Z}$ el conjunto de variables evidencia cuyo valor ha sido observado.

\nocite{Koller2009}
\begin{definition}
\begin{itemize}
 \item  Una ruta se encuentra \emph{activa} si no tiene \emph{estructuras-v} (dos padres $X_{i-1},X_{i+1}$ de un nodo común $X_i$, $X_{i-1} \rightarrow X_{i} \leftarrow X_{i+1}$) [\fref{fig:rutas} (Izquierda)].
 
 \item Una ruta $X_1 - ... - X_k$ está \emph{activa dado $\mathbb{Z}$} si:
 \begin{itemize}
  \item Para cualquier \textit{estructura-v} $X_{i-1} \rightarrow X_i \leftarrow X_{i+1}$ se tiene que $X_i$ o alguno de sus descencientes están en $\mathbb{Z}$.
  \item Ningún otro $X_i$ está en $\mathbb{Z}$.
 \end{itemize}
\end{itemize}
\end{definition}


\begin{exercise}
 Dada la red Bayesiana en la \fref{fig:senderosActivos}, determinar si los senderos siguientes son activos, dado que la variable $G$ fue observada:\footnote{Ejericio tomado del curso en línea de Daphne Koller.}
 \begin{enumerate}
  \item $C \rightarrow D \rightarrow G \leftarrow I \rightarrow S$ \gtick
  \item $I \rightarrow G \rightarrow L \rightarrow J \rightarrow H$
  \item $I \rightarrow S \rightarrow J \rightarrow H$ \gtick
  \item $C \rightarrow D \rightarrow G \leftarrow I \rightarrow S \rightarrow J \leftarrow L$
 \end{enumerate}

\end{exercise}

\begin{figure}
\begin{center}
 \begin{tikzpicture}
  \node [oval] (C) {C};
  \node [oval, below of=C] (D) {D};
  \node [oval, cobaltNode, below right of=D] (G) {G};
  \node [oval, above right of=G] (I) {I};
  \node [oval, below of=G] (L) {L};
  \node [oval, below right of=I] (S) {S};
  \node [oval, below right of=L] (J) {J};
  \node [oval, below left of=L] (H) {H};
  \draw [-Stealth] (C) -- (D);
  \draw [-Stealth] (D) -- (G);
  \draw [-Stealth] (I) -- (G);
  \draw [-Stealth] (I) -- (S);
  \draw [-Stealth] (G) -- (L);
  \draw [-Stealth] (G) -- (H);
  \draw [-Stealth] (S) -- (J);
  \draw [-Stealth] (J) -- (H);
  \draw [-Stealth] (L) -- (J);
 \end{tikzpicture}
\end{center}
\caption{Ejemplo de gráfica de Bayes con $G$ observado.}\label{fig:senderosActivos}
\end{figure}

\subsubsection{D-separación}

\begin{definition}
 Dos variables aleatorias $X$ y $Y$ se encuentran \emph{d-separadas} en $G$ dado $\mathbb{Z}$ si no existe una ruta activa entre $X$ y $Y$ dado $\mathbb{Z}$.
 \begin{align}
  d-sep_G(X,Y|Z)
 \end{align}
\end{definition}

\begin{theorem}
 Si la distribución de probabilidad $P$ se factoriza sobre $G$ y $d-sep_G(X,Y|Z)$ entonces $P$ satisface $(X \perp Y | Z)$.
\end{theorem}

Cualquier nodo se encuentra $d-separado$ de sus no descendientes dados sus padres $\Rightarrow$ Si $P$ se factoriza sobre $G$, entonces en $P$ cualquier variable es independiente de sus no-descendientes dados sus padres.


\begin{example}
 Indique si existe una d-separación en los casos siguientes:
 \begin{enumerate}
  \item $d-sep(D,I|L)$
  \item $d-sep(D,J|L)$
  \item $d-sep(D,J|L,I)$ \gtick
  \item $d-sep(D,J|L,H,I)$
 \end{enumerate}

\end{example}

\begin{figure}
\begin{center}
 \begin{tikzpicture}
  \node [oval] (C) {C};
  \node [oval, below of=C] (D) {D};
  \node [oval, below right of=D] (G) {G};
  \node [oval, blackNode, above right of=G] (I) {I};
  \node [oval, cobaltNode, below of=G] (L) {L};
  \node [oval, below right of=I] (S) {S};
  \node [oval, below right of=L] (J) {J};
  \node [oval, greenNode, below left of=L] (H) {H};
  \draw [-Stealth] (C) -- (D);
  \draw [-Stealth] (D) -- (G);
  \draw [-Stealth] (I) -- (G);
  \draw [-Stealth] (I) -- (S);
  \draw [-Stealth] (G) -- (L);
  \draw [-Stealth] (G) -- (H);
  \draw [-Stealth] (S) -- (J);
  \draw [-Stealth] (J) -- (H);
  \draw [-Stealth] (L) -- (J);
 \end{tikzpicture}
\end{center}
\caption{Ejemplo de gráfica de Bayes con $I$, $L$ y $H$ observados. $I$ bloquea el flujo, $H$ lo habilita y $L$ varía su función dependiendo del sendero analizado.}\label{fig:senderosActivos1}
\end{figure}




\subsection{Mapas de independencias (Mapas-I)}

Si la distribución de probabilidades conjuntas $P$ satisface todas las relaciones de independencia $I$ implicadas por las $d-separaciones$ de una gráfica $G$ se dice que $G$ es un mapa de independencias de $P$.
\begin{align}
 I(G) = \{(X \perp Y | Z) : d-sep_G(X,Y|Z)\}
\end{align}
Los mapas de independencias para $P$ no son únicos y no necesariamente contienen todas las relaciones de independencia que son válidas en $P$.
Si $G$ es un mapa de independencias para $P$, entonces $P$ se factoriza sobre $G$, para demostrar esto se utiliza la regla de la cadena y el hecho de que cualquier nodo es independiente de sus no descendientes dados su padres.


\subsection{I-Equivalencia}

\begin{figure}
 \centering
 \begin{tikzpicture}[->,>=stealth']
  \node [ellipse, cobaltNode] (X) {$X$};
  \node [ellipse, cobaltNode, right = 5mm of X] (Y) {$Y$};
  \node [ellipse, cobaltNode, right = 5mm of Y] (Z) {$Z$};
  \draw (X) -- (Y);
  \draw (Y) -- (Z);
 \end{tikzpicture}
 \hspace*{1cm}
 \begin{tikzpicture}[->,>=stealth']
  \node [ellipse, cobaltNode] (X) {$X$};
  \node [ellipse, cobaltNode, right = 5mm of X] (Y) {$Y$};
  \node [ellipse, cobaltNode, right = 5mm of Y] (Z) {$Z$};
  \draw (Y) -- (X);
  \draw (Y) -- (Z);
 \end{tikzpicture}
 \hspace*{1cm}
 \begin{tikzpicture}[->,>=stealth']
  \node [ellipse, cobaltNode] (X) {$X$};
  \node [ellipse, cobaltNode, right = 5mm of X] (Y) {$Y$};
  \node [ellipse, cobaltNode, right = 5mm of Y] (Z) {$Z$};
  \draw (Y) -- (X);
  \draw (Z) -- (Y);
 \end{tikzpicture}
 
 \vspace*{1cm}
 \begin{tikzpicture}[->,>=stealth']
  \node [ellipse, cobaltNode] (X) {$X$};
  \node [ellipse, cobaltNode, right = 5mm of X] (Y) {$Y$};
  \node [ellipse, cobaltNode, right = 5mm of Y] (Z) {$Z$};
  \draw (X) -- (Y);
  \draw (Z) -- (Y);
 \end{tikzpicture}
 \caption{Arriba: Tres gráficas $I-equivalentes$.  Abajo: Gráfica no $I-Equivalente$ a las anteriores.}\label{fig:iequiv}
\end{figure}


Dos gráficas $G_1$ y $G_2$ sobre $X_1,...,X_n$ son $I-Equivalentes$ si los conjuntos de independencias derivadas de ambas gráficas son iguales \fref{fig:iequiv}.
\begin{align}
 I(G_1) = I(G_2)
\end{align}


\subsection{Inferencia}
\subsection{Eliminación de variables}
\subsection{Modelo Ingenuo de Bayes}

Es un modelo particular utilizado para problemas de clasificación, que asume que todas las características $C_i$ de una clase $Clase$ son independientes entre sí, dada información sobre la clase, esto es $\forall C_i,C_j(C_i \perp C_j|Clase)$ [\fref{fig:naiveBayes}], de tal modo que la factorización siguiente es válida:
\begin{align}
 P(Clase,C_1,...,C_n) = P(Clase) \prod^n_{i=1}P(C_i|Clase)
\end{align}


\begin{figure}
 \centering
 \begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 20mm/#1, level distance = 1.5cm}]
  \node [ellipse,violetNode]{$Clase$} [level distance=10mm,sibling distance=10mm]
    child { node [ellipse,cobaltNode] {$C_1$} }
    child { node [ellipse,cobaltNode] {$C_2$} }
    child { node {...} }
    child { node [ellipse,cobaltNode] {$C_n$} };
 \end{tikzpicture}
 \caption{En modelo Ingenuo de Bayes, las características de una clase son independientes, dada la clase.}\label{fig:naiveBayes}
\end{figure}



\subsection{Inferencia aproximada}
\subsubsection{Muestreo directo}
\subsubsection{Verosimilitud ponderada}
\subsubsection{Muestreo Montecarlo en cadenas de Makov (MCCM)}


\section{Redes Bayesianas Dinámicas}
{\color{red} Sección incompleta.}

Sea $X$ una variable aleatoria cuyo valor cambia con el tiempo.  La probabilidad de una secuencia de eventos que ocurren a tiempos discretos $t=\{0,...,T\}$ se expresa:
\begin{align*}
 P(X^{(0:T)}) = P(X^{(0)})\prod^{T-1}_{t=0}P(X^{(t+1)}|X^{(0:t)})
\end{align*}
utilizando la regla de la cadena.

La \emph{hipótesis de Márkov} asume que:
\begin{align}
 (X^{(t+1)} \perp X^{(0:t-1)}|X^{(t)})
\end{align}
entonces se cumple que:
\begin{align}
 P(X^{(0:T)}) = P(X^{(0)})\prod^{T-1}_{t=0}P(X^{(t+1)}|X^{(t)})
\end{align}

Otra simplificación que se puede hacer al modelo es asumir \emph{independencia temporal}, a lo cual también se le conoce como asumir que el sistema es \emph{estacionario}.  Esto se expresa:
\begin{align}
 P(X^{(t+1)}|X^{(t)}) &= P(X'|X) & \forall t
\end{align}
Es decir, la dinámica del sistema no cambia con el tiempo.

Un sistema puede estar descrito por un conjunto de variables para cada tiempo y las premisas de independecia se aplican sobre ellas.  Para aliviar las restricciones del modelo, estas variables pueden codificar información derivada de otras relaciones temporales.  Por ejemplo: en un problema de localización de un vehículo, se puede agregar a la velocidad como variable que describe el estado.

\cite{Ross2013, Bolstad2007}
%\bibliography{ianotesref}

%\end{document}
