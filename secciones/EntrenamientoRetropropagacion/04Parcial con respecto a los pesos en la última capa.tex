\section{Parcial con respecto a los pesos en la penúltima capa}


Una vez más para ser más legible los cálculos vamos a comenzar a calcular el gradiente.

\begin{figure}[H]
 \centering
 \includegraphics[scale=0.7]{../Figuras/AredNa.png}
 \caption{Red neuronal(entrenamiento última capa.)}
 \label{fig:graficaLog}
\end{figure}

Considerando que ahora solamente estamos evaluando a la red en un ejemplo y vamos a tener que resolverlo por etapas lo que estamos pensando es lo siguiente: nuestra función de la red neuronal, como aquí las entradas produjo, las etiquetas correspondientes y ahora lo que nuestra función de errores están viviendo es la distancia entre lo que obtuvimos en esta capa y una colección de etiquetas que era lo que nosotros queríamos que saliera. 

%$ \color{yellow} A = \textcolor{blue}{B} \mathbin{\textcolor{red}{-}} \textcolor{green}{C} $

Retomando la entropía cruzada \ref{entropiaCruzada} y enfocandonos en las entradas para la neurona de salida:
 \begin{equation}
  J (\Theta) = -\left[\sum_{k=1}^{s_{L}}y_{k}^{(i)} log(\textcolor{blue}{a_{k}^{(L)}}) + (1-y_{k}^{(i)}) log(1 - \textcolor{blue} {a_{k}^{(L)}})  \right] 
  \label{eq:EntropiaCruz}
 \end{equation}
 
 Entonces la capa de salida al derivar obtendriamos lo siguiente:
 \begin{equation}
  \dfrac{\partial J}{\partial \theta_{jk}^{L-1}} = - \left[ \dfrac{y_{k}}{a_{k}^{L}} \dfrac{\partial}{\partial\theta_{jk}^{(L-1)}} g(\textcolor{red}{s_{k}}) - \left(\dfrac{1 - y_{k}}{1 - a_{k}^{L}} \right) \dfrac{\partial}{\partial \theta_{jk}^{(L-1)}}g(z_{k})\right]
 \end{equation}
 
Con:
\begin{equation}
 \textcolor{red}{s_{k}} = \sum_{j'=0}^{S_{L-1}} \theta_{j'k}a_{j'}^{L-1}
\end{equation}

De la derivada de la suma sólo queda $a_{j'} $ con $j' = j$ .
\begin{equation}
 =-\left[ \dfrac{y_{k}}{a_{k}^{L}} g'(z_{k}) -\left( \dfrac{1-y_{k}}{1-a_{k}^{(L)}} \right) g'(z_{k})\right]a_{j}^{(L)}
\end{equation}

Recordando que $g' = g (1 - g)$ ahora tenemos que:
\begin{equation}
 =-\left[ \dfrac{y_{k}(1-a_{k}^{L})-a_{k}^{L}(1-y_{k})} {a_{k}^{L}(1-a_{k}^{L})} \right]\textcolor{blue}{g'(z_{k})} a_{j}^{(L)}
\end{equation}

Con:
\begin{equation}
 \textcolor{blue}{g'(z_{k})} = a_{k}^{(L)}(1-a_{k}^{L})
\end{equation}

Así nos queda como: 
\begin{equation}
 =-a_{j}^{(L-1)})(y_{k}-a_{k}^{(L)})
\end{equation}

Representando a $(y_{k}-a_{k}^{(L)})$ como $\delta_{k}$ nos queda.
\begin{equation}
 =-a_{j}^{(L-1)})\delta_{k}
\end{equation}

Entonces nuestra función de error está actuando directamente sobre esta capa nada más,  utilizando los pesos en esta capa, evaluado lo que había ocurrido en la capa anterior involucrando también a los pesos de esta otra capa. Entonces podemos pensar que los valores que obtuvimos aquí a la salida dependen de los pesos en cada una de las capas, sin embargo la dependencia es ligeramente distinta. En el sentido de que tenemos que irnos dos niveles más hacia atras, recordando que este influyó en el cálculo de ese número de salida.

Entonces el gradiente lo vamos a ir calculando capa a capa, porque de ahí viene precisamente el nombre de \emph{back propagation} entonces lo primero y lo más sencillo va a ser calcular como dependió la función de error de los pesos que se encuentran inmediatamente detrás de las neuronas de salida. En este momento es considerar que en el algoritmo \emph{feedforward} la primera capa la vamos a tomar con los índices y la siguiente capa índices j la tercera capa índices k por eso estamos pensando que los pesos de la última capa conectan a la neurona j con la neurona k pero para considerar que vamos a ir haciendo esto en reversa. 
Entonces a través un poco de notación s l es el número de neuronas que tenemos en la capa l s l menos 1 sería el número de neuronas en la capa l menos suelo y así sucesivamente y aquí viene la misma definición que ya teníamos antes el valor que queremos llegar y el valor que realmente obtuvimos de hecho podemos quitar ahorita este 6 porque solamente hay un ejemplar de entrenamiento entonces ya tenemos aquí un solo ejemplo y ahora lo que queremos hacer es calcular la derivada con respecto a los pesos justamente antes de haber evaluado el valor de la última capa la notación que estamos utilizando aquí en vez de la h de hipótesis es el hecho de que tal cual el valor de la hipótesis es el valor de activación de la neurona entonces estamos hablando del valor de activación de cada una de estas y esas son las áreas si se fijan cuando evaluamos el algoritmo feedforward ya obtuvimos los valores de activación de hecho de todas las neuronas entonces los podemos utilizar aquí y precisamente lo que vamos a hacer para poder calcular la derivada es recordar hecho eso de nuestro valor de activación fue calculado con las reglas de activación de un perceptor entonces comenzamos a calcular la derivada de la entropía cruzada en primer lugar vamos a observar que estoy calculando la parcial del error con respecto a uno de los pesos bueno aquí tenemos la suma sobre todo de las neuronas de salida no pero observemos qué ocurre con este peso solamente está contribuyendo a la salida de esta neurona a esta neurona no le afecta en lo más mínimo lo que haya ocurrido aquí y esta neurona tampoco le afecta luego entonces la parcial de estos dos elementos con respecto a este peso vale cero porque no hay una dependencia son constantes en este caso si para la neurona que está conectada con este peso es decir la neurona acá sí importa lo que ocurrió con este peso por ello de los tres elementos que teníamos aquí aparece en el dibujo es el en general solamente va a sobrevivir uno de los términos y es aquel donde estamos trabajando exactamente con la neurona carísima entonces si de momento ya no tenemos esta suma podemos trabajar solamente con lo que hay aquí adentro tenemos nuestro signo menos acá afuera después llegué acá 1 - que realmente son constantes.
Pues que es la función logística por 1 - ella misma es esa primera parte la vamos a dejar aquí indicada por otro lado tenemos que continuar con la regla de la cadena calculando la derivada de esta suma que tenemos aquí adentro pero lo que acabamos de ver es que solamente va a sobrevivir pues el elemento donde jota prima es jota y los otros se van a morir son constantes con respecto a este peso entonces la derivada de este elemento con respecto a éste es uno y el término que queda sobreviviendo es esta y es precisamente entonces le estoy poniendo aquí afuera de una vez porque nos va a salir otro idéntico del lado derecho recuerdan de la x que teníamos cuando calculamos la deriva de la signo y bueno aquí está precisamente el término que está sobreviviendo muy bien entonces repitiendo la misma idea de este lado tenemos el mismo factor tenemos que prima y también aquí al aplicar la regla de la cadena va a salir una idénticamente entonces por eso le estaba factor izando de una vez lo siguiente que vamos a hacer es bueno factorizar este también lo estamos poniendo aquí afuera y otra vez lo que vamos a hacer es trabajar con estos dos para tratar de escribirlos de una manera más amigable ponemos entonces como un denominador estos dos se multiplican este que está aquí lo multiplicó por el denominador de acá estoy acá lo multiplicó por el de acá tenemos el signo menos los voy a llegar por 1 menos acá menos acá por un número chica una vez que hacemos esto las cosas se vuelven hermosas esto de aquí es rica esto de acá es un término cruzado allí - una al menos por menos más un término cruzado allí éste se va con éste. 
Ahora tenemos que recordar también que propiedad  tenía la derivada prima pues era g por uno menos 100 pero la función g evaluada en la combinación lineal que es pues es exactamente la que calculamos cuando estábamos haciendo el fit for work así salió entonces este es que es que tengo aquí le sustituyó simple y sencillamente por la sas que fue lo que calculamos en el instante en el que recuerdo esto o miren esteban y fue los que nos quedó entonces pues únicamente la diferencia entre lo que quería y lo que salió multiplicado por el valor de activación de la neurona en la capa anterior y ya tenemos entonces la manera de calcular todas las parciales del error con respecto a los pesos y en esta capa vamos a ver entonces en el próximo capítulo cómo calcular las derivadas con respecto a los pesos que están en la capacidad.
