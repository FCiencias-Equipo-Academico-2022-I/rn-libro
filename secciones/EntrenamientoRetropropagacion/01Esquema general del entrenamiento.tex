\section{Esquema general entrenamiento}
 
 En este capítulo vamos a trabajar con el entrenamiento (específicamente por retropropagación), en esta sección abordaremos primero el esquema general del entrenamiento. 
 A lo largo del capítulo veremos la derivación del primer algoritmo de entrenamiento para perceptrones multicapa (este es de los más comunes en esta área y el primero descrito propiamente).

En el tercer capítulo habíamos visto el concepto de aprendizaje para una máquina, ahora vamos a definir en que consiste aprender algo en el campo de las redes neuronales. Retomando lo descrito en ese capítulo, recordemos que aprender para un modelo es más que nada encontrar una función que se asemeje a ciertas salidas específicas, está función está dentro de un espacio de funciones posibles propuestas. La función que deseamos encontrar modela algún tipo de tarea que nos interesa que se aprenda. 

\begin{definition}
 \textbf{El problema de aprendizaje para un perceptrón multicapa:} Dada una arquitectura de red neuronal, encontrar los pesos tales que la función $h\Theta(\vec{x}) $ (definida por la red neuronal) aproxime, hasta cierta tolerancia, la función de interés $ f(\vec{x})$. \\ Con $\vec{x}$ los datos de entrada.
 
\end{definition}

En la definición anterior, el vector $\vec{x}$, consisten en distintas características de los ejemplares provistos, según sea el problema a resolver, por ejemplo:
\begin{itemize}
 \item Segmentos de alguna canción.
 \item Píxeles de imágenes.
 \item Valores equivalentes a precios del mercado.
\end{itemize}

En este momento nos estamos concentrando en problemas de clasificación así que estás redes estan clasificando los ejemplares según ciertas características. Estas clasificaciones pueden ser muy interesantes, algunas por ejemplo se pueden dedicar a clasificar canciones, identificando ciertas emociones que transmiten como tristeza o alegria. 

Nuestra red neuronal va empezar sus calculos apartir de los datos de entrada $\vec{x}$, entonces al finalizar cada uno de los ejemplares va a ser mapeado a alguna clase (en el espacio de clases). Por medio de una función donde $f(x)$ es nuestra función ideal que hacer el mapeo exacto los ejemplares a su clase. Así si logramos que nuestra función $h\Theta(\vec{x})$ se aproxime a $f(x)$, la red neuronal nos daría las respuestas esperadas dado ciertos ejemplares, como por ejemplo:
\begin{itemize}
 \item Dado unas imagenes de distintas plantas, identificar correctamente el nombre de cada una.
 \item Dado una fragmento de canción indicarnos su genero.
 \item Dada una oración identificar si contiene un mensaje de odio.
\end{itemize}

Entonces el reto de la red neuronal es aproximar una función $f(x)$ (que se asume existe en el universo),
primero debemos intentar calcular $h\Theta(\vec{x})$ (\emph{h} de hipótesis, de un espacio de hipótesis). Está hipotesis puede que aproxime muy bien la función pero en la mayoria de los casos no es así, entonces iremos haciendo ajustes a los pesos (que definen la función h) para que pueda aproximarse lo más posible a nuestra función objetivo.


Como en muchos casos vamos a intentar aproximar funciones que no corresponden a un concepto matemático, sino más a percepciones humanos tales como sentimientos o clasificar abstraciones humanas, entonces se puede dar el caso que realemente no se pueda aproximar con total exactitud la función y le demos un cierto \emph{grado de tolerancia}, donde se asume que se pueda dar ciertos errores en ciertos ejemplares. Dependiendo del contexto vamos a asignar el rango de tolerancia de errores.

Es importante notar que si los datos de entrenamiento resultan clasificados con mucha precisión es probable que la red este aprendiendo datos no utiles para la tarea dada, es decir ese aprendiendo \emph{ruido}. Y al momento pasar los ejemplares de prueba no entregue buenos restultados.

Eentrenar una red para que se aproxime a la función objetivo cada vez más, debemos hacer lo siguiente:
\begin{itemize}
 \item  Definir una \emph{función de error} o pérdida $J$ (o $L$ de lost, perdida en íngles) que mida la distancia entre los valores deseados y los valores obtenidos con un conjunto de pesos dados $\Theta$ .
 \item Utilizar alguna \emph{técnica de optimización} de funciones que minimize este error.
 \item Definir la \emph{arquitectura} de la red, la \emph{función de error} y la técnica de
optimización, conciderando el tipo de función objetivo.
\end{itemize}

Una vez que ya sabemos en que consiste el entrenamiendo veamos un esquema general conocido como \emph{entranamiento por retropropagación} o propagación hacia atras, en íngles \textit{Backpropagation}. Anteriormente habíamos visto la evalución de una red con propagación hacia delante (de izquierda a derecha), con la retroprogación vamos a recorrer la red hacia atras (de derecha a izquierda), de la siguiente forma:
\begin{itemize}
 \item Utilizar \emph{la regla de la cadena} para obtener el gradiente de la función de error con
respecto al conjunto de pesos, dado un conjunto de datos de entrenamiento \emph{$X$} con sus etiquetas \emph{$Y$}.
\item Utilizar \emph{descenso por el gradiente} para encontrar un mínimo, lo suficientemente
bueno, de la función de error. Está técnica es la más sencilla, por eso usualmente se utilizaba en un principio
\begin{itemize}
 \item La idea es que al inicio no sabemos cuánto deben de valer los pesos para modelar correctamente la función, entonces 
 al comienzo se hace una asignación aleatoria de pesos. Existe un conjunto de pesos ideales a los cuales queremos aproximar nuestros pesos aleatorios. Con estos calculamos el error que tiene nuestros pesos, haciendo las comparaciones con los datos que se obtuvieron al aplicar el feedforward, sobre los datos de entrada con los datos esperados. El desenso por el gradiente va modificar poco a poco los pesos, para tengamos una nueva (y mejor) aproximación y nos acerquemos más a una región donde el error sea menor.
 Para calcular el descenso por el gradiente, se calculan las derivadas de la función de error con respecto a los parámetros. 
 \item Entonces tenemos una actualización de la siguiente forma: 
    \begin{equation}
     Theta' = Theta - \alpha \nabla_{\Theta}J
    \end{equation}
        Donde \emph{Theta} es la propuesta para nuestros parametros iniciales, $\nabla_{\Theta}$ es el gradiente que nos da el máximo acenso de la función de error, el cúal lo invertimos con el menos, para que siempre nos dirijamos al minimo local más proximo.  
\end{itemize}

\item Se puede usar otra técnica de optimización más avanzada, pero esta también involucraría al grandiente. 

\item La derivación original utilizaba diferencias al cuadrado como función de error. Esta la pueden consultar en 
Russell, Stuart y Peter Norving (2010). Artificial Intelligence, A Modern Approachs.

\item Para problemas de clasificación es más efectivo usar entropía cruzada como método de optimización. Para problemas de regresión es adecuado usar diferencias al cuadrado.
\end{itemize}
