\section{Parcial con respecto a los pesos en la última capa}

\begin{figure}[H]
 \centering
 \includegraphics[scale=0.8]{../Figuras/AredN.png}
 %\caption{última capa.}
 \label{fig:graficaLog}
\end{figure}


En esta sección vamos trabajar con la última capa, vamos a tener un poco más de elementos pero va a ser sencillo agregarlos en primer lugar vamos a tener que observar quienes se ven afectados por el nuevo peso con respecto al cual queremos derivar bien aquí tenemos un poco más de transmisión de efecto este peso solamente afecta a esta neurona pero a través de esta neurona. ahora si estamos conectados con absolutamente todas las otras neuronas.

Retomando la entroía cruzada de la sección anterior \ref{eq:EntropiaCruz}

 \begin{equation}
  J (\Theta) = -\left[\sum_{k=1}^{s_{L}}y_{k}^{(i)} log(\textcolor{blue}{a_{k}^{(L)}}) + (1-y_{k}^{(i)}) log(1 - \textcolor{blue} {a_{k}^{(L)}})  \right] 
  \tag{\ref{eq:EntropiaCruz}}
 \end{equation}
 
 Capa anterior:
  \begin{equation}
  \dfrac{\partial J}{\partial \theta_{jk}^{L-2}} = - \sum_{k=1}^{S_{L}}\left[ \dfrac{y_{k}}{a_{k}^{L}} \dfrac{\partial}{\partial\theta_{jk}^{(L-2)}} g(\textcolor{blue}{z_{k}}) - \left(\dfrac{1 - y_{k}}{1 - a_{k}^{L}} \right) \dfrac{\partial}{\partial \theta_{jk}^{(L-2)}}g( \textcolor{blue}{z_{k}} )\right]
 \end{equation}

 Con:
 \begin{equation}
  \textcolor{blue}{z_{k}} = \sum_{j'=0}^{S_{L-1}}\theta_{j'k} a_{j'}^{L-1}
 \end{equation}

 Entonces tenemos que:
 \begin{equation}
 \dfrac{\partial J}{\partial \theta_{jk}^{L-2}}  =-\sum_{k=1}^{S_{L}}(y_{k}-a_{k}^{L})\theta_{jk}\dfrac{\delta}{\delta\theta_{Lj}^{(L-2)}} \textcolor{blue}{a_j^{(L-1)}}
 \end{equation}

Sustituyendo a $\color{blue} (y_{k}-a_{k}^{L}) = \delta_{k}$ y a $\color{blue}{a_j^{(L-1)}}= g(z_{j})$ nos queda: 

 \begin{equation}
  \dfrac{\partial J}{\partial \theta_{jk}^{L-2}} =-\sum_{k=1}^{S_{L}}(\delta_{k})\theta_{jk}\dfrac{\delta}{\delta\theta_{Lj}^{(L-2)}} g(\textcolor{blue}{z_{j}})
 \end{equation}
 
 Con:
 \begin{equation}
  \textcolor{blue}{z_{j}} = \sum_{i'=0}^{S_{(L-2)}}\theta_{i'j}^{L-2} a_{i'}^{(L-2)}
 \end{equation}

 Así finalmente tenemos que:
 \begin{equation}
 \dfrac{\partial J}{\partial \theta_{jk}^{L-2}} = - \textcolor{blue}{\sum_{k=1}^{S_{L}} \delta_{k}\theta_{jk}g'(z_{j})} a_{i}^{(L-2)} =  \textcolor{blue}{\delta_{j}^{(L-1)}} a_{i}^{(L-2)}
 \end{equation}

 

Entonces aquí vamos a tener que hacer los toman en cuenta más términos clientes para poder hacer esta derivación voy a tener que volver a empezar desde un principio es decir vamos a volver a empezar con la función de error original y vamos a tener que empezar a derivar desde aquí vamos a recordar ahora que cada vez que calculamos los valores de activación en la última capa bueno estos provienen en realidad también dependen de el valor de activación en esta neurona de acá atrás entonces eventualmente vamos a tener que regresar hasta acá y el valor de esta neurona pues dependió ahora sí directamente del peso que nos está interesando entonces vamos a ir desarrollando poco a poco esa composición de funciones entonces comencemos con el mismo paso de antes calculamos la derivada parcial de jota con respecto al peso correspondiente pero ahora si no puedo eliminar la suma que tengo al inicio porque todos los valores de salida dependen de el peso con el que estamos trabajando entonces lo que vamos a tener aquí ahorita es que nos quedamos con la suma lo único que estoy haciendo ahora es pasarla junto con el signo menos recordemos que la derivada de una suma es la suma de las derivadas entonces podemos meter inmediatamente nuestro problema de derivar a la parte de adentro y simplemente dejar la suma que fuera entonces la primera parte se ve idéntica que antes esto es una constante derivada del logaritmo eso no entre acá para poder calcular esta parcial entonces tenemos que recordar como estaba escrita y lo mismo va a ocurrir de este lado va a salir del signo menos por lo que teníamos acá adentro y tenemos entonces 1 - entre 1 - acá por la parcial de estar acá es lo que estamos sustituyendo acá ya hicimos todas las cuentas todo lo que se cancela entonces no es necesario volverlo a hacer simple y sencillamente llegamos a lo que ya teníamos antes era ye menos llega a menos acá todo esto a lo que se le había llamado delta acá y lo que vamos a empezar ahora es a sacar un poquito los detalles estábamos antes derivando con respecto antes de vestido con respecto a jk y solamente nos había quedado un término aquí en este caso estamos derivando con respecto a y jota y quien depende de y jota pues son las as es básicamente estos pesos son constantes. Entonces de la primera capa que el error cometido por la última capa lo podíamos ver directamente como la diferencia entre lo que queríamos y lo que logramos calcular esto tiene una característica bastante interesante es exactamente por cada neurona tenemos uno de estos después venía la parcial de j con respecto a cada uno de los pesos observemos que por cada uno de estos errores vamos a tener de hecho tantos componentes de éstas como pesos estaban conectados con la carísima neurona entonces de estos tenemos un montón y lo único que teníamos que hacer era multiplicar este valor único por el valor de activación de la neurona con la cual estaba conectado y bueno aquí el signo menos que venimos cargando por la definición de la función de error y ya está ahora equipo se ve con estas tres neuronas bien en la definición de este delta j donde tenemos un producto de las delta casa que venían capa que está más hacia adelante multiplicados por los pesos correspondientes tenemos una suma sobre todos los elementos en última capa y una vez que hicimos esto viene multiplicar por función prima evaluada en la receta j observemos una vez más que está nada más depende de jota no depende de las casas entonces por eso lo podríamos poner entonces realmente le suma nada más afecta este ya que tenemos entonces este producto este que está aquí.

Resumiendo tenemos:
\begin{equation}
 \delta_{k}^{(L)} = (y_{k}-a_{k}^{(L)})
\end{equation}

\begin{equation}
 \delta_{j}^{(L-1)} = \left( \sum_{k=1}^{S_{L}} \delta_{k}^{(L)} \theta_{jk}\right)g'(z_j)
\end{equation}

Y sus respectivas derivadas:
\begin{equation}
 \dfrac{\partial J}{\partial \theta_{jk}^{(L-1)}} = -a_{j}^{(L-1)}\delta_{k}^{(L)}
\end{equation}

\begin{equation}
 \dfrac{\partial J}{\partial \theta_{ij}^{(L-2)}} = -a_{j}^{(L-2)}\delta_{j}^{(L-1)}
\end{equation}

Otra vez tenemos uno por cada neurona en la capa de en medio entonces aquí tenemos las casas aunque vamos a tener las jotas y tenemos uno por cada una de estas este se suele interpretar también como la contribución el error o el error que cometieron todas las neuronas en la capa de enmedio y observen que el error de cada neurona pues realmente tiene una es la suma de como contribuyó al error de todas las neuronas que estaban en la capa siguiente pues tiene mucho sentido no se están participando en todos lados pues su error es la suma de todos los errores a los cuales contribuyó y entonces para calcular el gradiente otra vez nos queda una fórmula bastante sencilla es multiplicar este único error por el valor de activación de la neurona con la cual estaba conectada en la capa anterior y de esta manera podemos obtener todas las parciales con respecto a los pesos que estaban conectados con estas neuronas y aquí tenemos todos esos y este que quedaría la parte de hacer el cálculo directamente el siguiente problema que vamos a tener es bueno si podríamos implementar perfectamente ya con un algoritmo de descenso por el gradiente con estas fórmulas que tenemos aquí. Simplemente tendremos un montón de ciclos for para estar calculando todas estas sumas y multiplicaciones sin embargo la forma en la que se acostumbra a trabajar ahora con las redes neuronales no es directamente calculando esto componente por componente sino que lo vamos a utilizar con notación matricial esto va a tener varias ventajas por un lado la notación va a ser muchísimo más compacta y por otro lado va a permitir en la implementación de los algoritmos con procesadores con gpu de manera que estas operaciones se están realizando en paralelo y entonces trabajamos muchísimo más rápido con las redes neuronales y realmente el estar utilizando notación matricial va a tener un impacto directo sobre el tiempo que tardan en ejecutarse nuestros algoritmos. 
