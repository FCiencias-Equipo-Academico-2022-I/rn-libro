\documentclass{article}

\usepackage[spanish]{babel}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=2cm,marginparwidth=1.75cm]{geometry}

% Useful packages
%\usepackage{amsmath}
%\usepackage{graphicx}
%\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Redes Neuronales - Libro \\ Índice}
\author{Veronica E. Arriola-Rios \\ Fer J. Gtz.}
\date{08/Enero/2022}

\begin{document}
\maketitle

\section{Neurona biológica}
\subsection{Sistema nervioso}
\subsection{Neurona biológica}
\subsection{Modelo de Hodgkin-Huxley: membrana y canal}
\subsection{Ecuaciones diferenciales}
\section{Hodgkin-Huxley} % === CAPITULO 2  ===
\subsection{Modelo de Hodgkin-Huxley. Dinámica de los disparos}
\subsection{Método de Euler}
\subsection{Hodgkin-Huxley en IPython notebook}
\section{Aprendizaje de máquina} % === CAPITULO 3  === 
\subsection{Espacio de hipótesis}
\subsection{Conjuntos de entrenaiento, validación y prueba}
\subsection{Perceptrón}
\subsection{Compuertas lógicas con el perceptrón}
\subsection{Funciones de activación}
\subsection{Funciones de error: diferencias al cuadrado y entropía cruzada}
\subsection{Medidas de rendimiento: Matriz de confusión, precisión, recall, f score, etc}
\section{ Perceptrón multicapa} % === CAPITULO 4  === 
\subsection{XOR}
\subsection{Propagación hacia adelante manual}
\subsection{Propagación hacia adelante vectorizada (con matrices)}
\subsection{Expresividad de la hipótesis, dependencia de las neuronas en la capa de en medio}
\subsection{Teorema del aproximador universal (Michale Nielsen)}
\section{Entrenamiento} % === CAPITULO 5  ===
\subsection{Retropropagación. Gradiente de la función de error}
\subsection{Descenso por el gradiente}
\subsection{Otras funciones de optimización}
\section{Optimización del entrenamiento} % === CAPITULO 6  ===
\subsection{Redes Profundas}
\subsection{Gradiente desvaneciente (o que explota) }
\newpage
\subsection{Entrenamiento en línea vs en lotes}

\subsection{Normalización y normalización por lotes}

\subsection{Regularización}
\section{Casos Análisis e interpretación}% === CAPITULO 7  ===

\subsection{Red Hinton árbol familiar con numpy (entrenamiento)}

\subsection{Red Hinton árbol familiar con pytorch}
\section{AÚN NO TIENE NOMBRE}% === CAPITULO 8  ===
\subsection{MNIST versión básica con numpy}
\subsection{Neuroevolución}
\subsubsection{Entrenamiento con algoritmos genéticos}
\subsubsection{Neuroevolución profunda}
\section{Mapeos autoorganizados}% === CAPITULO 9  ===
\subsection{Mapeos autoo-organizados, Kohonen}
\section{Redes Neuronales Convolucionales}% === CAPITULO 10  ===
\section{Redes Neuronales Recurrentes}% === CAPITULO 11  ===
\subsection{Parciales ordenadas (Werbos)}
\subsection{Intro RNR. Sistemas dinámicos}
\subsection{Red 3}
\subsection{Redes 1 y 2 para clasificación}
\subsection{Predicción de secuencias, hasta bidireccionales}
\subsection{Redes traductoras, hasta recursivas}
\subsection{Inicio cómputo de yacimiento} % esto esta entrecomillas
\section{Atención}% === CAPITULO 12  ===
\subsection{Casos de análisis de serie}
\subsection{LSTM}
\subsection{GRU}
\subsection{Aplicaciones: ejemplos de RNR con git de cvicom: etiquetado de palabras y conjugación de verbos}
\section{Redes de Hopfield}% === CAPITULO 13  ===
\subsection{Redes de hopfield}
\subsection{Máquinas de Boltzman}

\newpage
\section{AÚN NO TIENE NOMBRE}% === CAPITULO 14  ===
\subsection{Entrenamiento}
\subsection{Partículas y partículas de fantasía}
\subsection{Máquinas de Boltzman Restringidas}
\section{Redes adversarias}% === CAPITULO 15  ===
\subsection{GANs}

\end{document}

